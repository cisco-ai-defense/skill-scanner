{
  "repo_notes": [
    {
      "content": "Skill Scanner is a security analysis tool that detects threats in AI Agent Skills packages (OpenAI Codex Skills and Cursor Agent Skills formats). It uses a defense-in-depth approach with six independent analyzer engines: pattern-based (YAML regex + YARA), Python AST dataflow analysis, LLM semantic analysis (supporting Anthropic, OpenAI, LiteLLM, Google GenAI, AWS Bedrock, GCP Vertex AI, Azure OpenAI), a meta-analyzer for false-positive filtering, VirusTotal hash-based malware detection, and Cisco AI Defense cloud-based threat detection. The threat taxonomy aligns with four frameworks: Cisco Integrated AI Security Framework (AITech codes), MITRE ATLAS, OWASP Top 10 for LLM & Agentic AI, and NIST AI RMF. All findings are mapped to these frameworks.",
      "author": "Maintainer"
    },
    {
      "content": "The codebase is organized into these key areas:\n- skill_scanner/cli/ — CLI entry point (click-based) with commands: scan, scan-all, list-analyzers, validate-rules, generate-policy, configure-policy (Textual TUI)\n- skill_scanner/api/ — FastAPI REST API server with endpoints for single scan, upload scan, batch scan, health, and analyzer listing\n- skill_scanner/core/ — Core scanning engine: scanner.py (SkillScanner orchestrator), loader.py (SKILL.md parser), models.py (Pydantic data models), scan_policy.py (policy engine)\n- skill_scanner/core/analyzers/ — All analyzer implementations: static.py, behavioral_analyzer.py, llm_analyzer.py, pipeline_analyzer.py, meta_analyzer.py, virustotal_analyzer.py, aidefense_analyzer.py, trigger_analyzer.py, bytecode_analyzer.py\n- skill_scanner/core/static_analysis/ — Python AST analysis: cfg_builder.py, dataflow_tracker.py, taint_tracker.py, bash_taint_tracker.py, semantic_analyzer.py\n- skill_scanner/core/reporters/ — Output formatters: json_reporter.py, sarif_reporter.py, markdown_reporter.py, table_reporter.py\n- skill_scanner/data/ — Detection rules: rules/signatures.yaml (58+ regex rules), yara_rules/ (14 YARA rule files), prompts/ (LLM prompt templates), default/strict/permissive policy presets\n- skill_scanner/threats/ — Threat taxonomy: threats.py (ThreatMapping), cisco_ai_taxonomy.py (AITech codes)\n- skill_scanner/hooks/ — Git pre-commit hook integration\n- skill_scanner/config/ — Configuration management, environment variables, constants",
      "author": "Maintainer"
    },
    {
      "content": "Documentation should prioritize practical usage: installation via uv or pip, CLI commands with real examples, Python SDK programmatic usage, REST API integration, CI/CD pipeline setup with GitHub Actions and SARIF output, pre-commit hook configuration, and scan policy customization. The examples/ directory contains 8 runnable Python scripts covering basic scanning, advanced multi-analyzer setups, batch scanning, behavioral analysis, LLM analysis, API usage, CI/CD integration, and programmatic usage. The evals/ directory contains a benchmarking framework with 10+ eval skills across threat categories (backdoor, command-injection, data-exfiltration, obfuscation, prompt-injection, etc.) and 10 policy presets for policy benchmarking. The docs/ directory has detailed markdown docs on architecture, each analyzer, scan policies, the API server, threat taxonomy, and development setup.",
      "author": "Maintainer"
    },
    {
      "content": "Key integration points: The package is published as cisco-ai-skill-scanner on PyPI. Entry points are skill-scanner (CLI), skill-scanner-api (API server), and skill-scanner-pre-commit (Git hook). Optional extras: [bedrock] for AWS Bedrock, [vertex] for GCP Vertex AI, [azure] for Azure OpenAI, [supply-chain] for guarddog, [all] for everything. The scanner supports configurable scan policies with three presets (strict, balanced, permissive) and a Textual-based TUI for interactive policy configuration. Output formats include summary (colored console), JSON, Markdown, ASCII table, and SARIF 2.1.0 for GitHub Code Scanning integration. The --fail-on-findings flag enables CI/CD build failure on HIGH/CRITICAL findings.",
      "author": "Maintainer"
    }
  ],
  "pages": [
    {
      "title": "Home",
      "purpose": "High-level overview of Skill Scanner: what it is, what problems it solves, key features (multi-engine detection, 6 analyzers, 4 security framework alignments), supported skill formats (OpenAI Codex, Cursor Agent Skills), and navigation guide to the rest of the wiki. Reference README.md and pyproject.toml."
    },
    {
      "title": "Key Concepts and Security Model",
      "purpose": "Explain foundational concepts: what AI Agent Skills are (SKILL.md format with YAML front matter, metadata, instructions, supporting files), why they pose security risks (arbitrary code execution, transitive trust, privilege escalation), the difference between skills and MCP servers, the defense-in-depth detection philosophy, the threat model, security best practices (recommended analyzer combinations for dev/staging/prod, binary handling rationale, remediation workflow), and limitations of each analyzer. Reference docs/, skill_scanner/threats/, docs/binary-handling.md, docs/remote-skills-analysis.md.",
      "parent": "Home"
    },
    {
      "title": "Quick Start",
      "purpose": "Step-by-step getting started guide: install with uv pip install cisco-ai-skill-scanner or pip, run your first scan with skill-scanner scan <path>, interpret the output, enable additional analyzers (--use-behavioral, --use-llm), set up environment variables for LLM API keys. Include real command examples and expected output. Reference docs/quickstart.md and README.md.",
      "parent": "Home"
    },
    {
      "title": "User Guide",
      "purpose": "Parent page for all user-facing documentation. Overview of the four ways to use Skill Scanner: CLI, Python SDK, REST API, and pre-commit hooks. Brief summary of each with links to child pages.",
      "parent": "Home"
    },
    {
      "title": "Installation and Configuration",
      "purpose": "Comprehensive installation guide: uv and pip installation, optional extras ([bedrock], [vertex], [azure], [supply-chain], [all]), all environment variables (SKILL_SCANNER_LLM_API_KEY, SKILL_SCANNER_LLM_MODEL, SKILL_SCANNER_LLM_PROVIDER, VIRUSTOTAL_API_KEY, AI_DEFENSE_API_KEY, AWS_REGION, AZURE_OPENAI_ENDPOINT, etc.), .env file setup using .env.example, configuration priority (CLI args > env vars > defaults), and LLM provider configuration for each supported provider (Anthropic, OpenAI, LiteLLM, Google GenAI, Bedrock, Vertex AI, Azure). Reference skill_scanner/config/config.py, .env.example.",
      "parent": "User Guide"
    },
    {
      "title": "CLI Usage",
      "purpose": "Complete CLI reference with examples for every command: (1) skill-scanner scan <dir> with all flags (--format, --output, --fail-on-findings, --use-behavioral, --use-llm, --use-virustotal, --use-aidefense, --enable-meta, --use-trigger, --policy, --custom-rules, --llm-provider, --llm-consensus-runs), (2) skill-scanner scan-all <dir> with --recursive and --check-overlap, (3) skill-scanner list-analyzers, (4) skill-scanner validate-rules, (5) skill-scanner generate-policy with --preset and -o, (6) skill-scanner configure-policy TUI. Show real command examples and expected outputs for each. Reference skill_scanner/cli/cli.py.",
      "parent": "User Guide"
    },
    {
      "title": "Python SDK",
      "purpose": "How to use Skill Scanner as a Python library: importing SkillScanner, scan_skill(), scan_directory(), load_skill(), configuring analyzers programmatically (StaticAnalyzer, BehavioralAnalyzer, LLMAnalyzer, etc.), accessing ScanResult fields (is_safe, findings, max_severity, scan_duration_seconds), filtering findings by severity and category, custom analyzer configurations, and working with Finding objects. Include code examples from examples/programmatic_usage.py and examples/basic_scan.py. Reference skill_scanner/core/scanner.py, skill_scanner/core/models.py.",
      "parent": "User Guide"
    },
    {
      "title": "API Server",
      "purpose": "Complete REST API documentation: starting the server (skill-scanner-api --host --port --reload), all endpoints with request/response schemas (GET /, GET /health, POST /scan with ScanRequest body, POST /scan-upload for ZIP files, POST /scan-batch for background batch scans, GET /scan-batch/{id} for status, GET /analyzers), authentication considerations, Docker deployment example, and API usage examples with curl and httpx. Reference skill_scanner/api/router.py, skill_scanner/api/api_server.py, docs/api-server.md, examples/api_usage.py.",
      "parent": "User Guide"
    },
    {
      "title": "Pre-commit Hooks",
      "purpose": "Git pre-commit hook setup and configuration: installing with skill-scanner-pre-commit install, .pre-commit-config.yaml setup, .skill_scannerrc configuration file (severity threshold, skills_path, fail_fast, use_behavioral, use_trigger), how staged files are detected and matched to skills, severity threshold behavior, and integration with existing pre-commit frameworks. Reference skill_scanner/hooks/pre_commit.py, .pre-commit-config.yaml.",
      "parent": "User Guide"
    },
    {
      "title": "Scan Policies",
      "purpose": "Comprehensive guide to the scan policy system: what policies control (hidden files, pipeline analysis, rule scoping, credentials, file classification, file limits, analysis thresholds, sensitive files, command safety, analyzer toggles, severity overrides, disabled rules, rule properties), the three presets (strict, balanced, permissive) with comparison table, generating policies with skill-scanner generate-policy, editing with configure-policy TUI, custom policy YAML format with all sections explained, using --policy flag, and policy benchmarking with the eval framework. Include all 10 eval policy presets as examples. Reference skill_scanner/core/scan_policy.py, skill_scanner/data/default_policy.yaml, docs/scan-policy.md, evals/policies/.",
      "parent": "User Guide"
    },
    {
      "title": "Architecture",
      "purpose": "Parent page for architecture documentation. High-level summary of the system design philosophy: modular analyzers, defense-in-depth, configurable pipeline, extensible plugin architecture, and the relationship between components.",
      "parent": "Home"
    },
    {
      "title": "System Overview",
      "purpose": "Architecture overview showing the four layers: Entry Points (CLI, API, pre-commit, SDK), Core Orchestration (SkillScanner, SkillLoader, Config), Analysis Engines (6 analyzers), and Output Reporting (5 reporters). Show how data flows from skill input through loading, analysis, aggregation, optional meta-analysis, to result construction. Include the data models: Skill, Finding, ScanResult, ScanPolicy, Severity enum, ThreatCategory enum. Cover SKILL.md parsing with python-frontmatter, ZIP archive handling, and the SkillLoader class. Reference skill_scanner/core/scanner.py, skill_scanner/core/models.py, skill_scanner/core/loader.py, docs/architecture.md.",
      "parent": "Architecture"
    },
    {
      "title": "Scanning Pipeline",
      "purpose": "Detailed walkthrough of the scanning pipeline stages: (1) Loading — SkillLoader parses SKILL.md with python-frontmatter, extracts metadata and files, handles ZIP archives; (2) Static Analysis — 6-pass architecture (metadata validation, instruction scanning, code scanning, cross-validation, file existence, YARA); (3) Parallel Analysis — behavioral AST, LLM semantic, external APIs run concurrently; (4) Aggregation — deduplication by threat_category+description hash, severity normalization; (5) Meta-Analysis — optional LLM-based false positive filtering; (6) Result Construction — ScanResult with is_safe, findings, max_severity, scan_duration_seconds. Reference skill_scanner/core/scanner.py, skill_scanner/core/loader.py.",
      "parent": "Architecture"
    },
    {
      "title": "Analyzers",
      "purpose": "Parent page listing all analyzer engines with a capabilities matrix: which are always available vs require API keys, detection methods, primary targets, and configuration flags. Overview of the Analyzer base interface and how analyzers produce Finding objects. Include the AnalyzerFactory and how analyzers are built from CLI flags and policy. Reference skill_scanner/core/analyzer_factory.py.",
      "parent": "Architecture"
    },
    {
      "title": "Static Analyzer",
      "purpose": "Deep dive into the StaticAnalyzer: YAML regex rules (signatures.yaml with 58+ patterns organized by category — prompt_injection, command_injection, path_traversal, sql_injection, hardcoded_secrets, obfuscation, etc.), YARA rule files (14 rules covering prompt injection, command injection, credential harvesting, code execution, SQL injection, obfuscation, autonomy abuse, capability inflation, tool chaining, system manipulation, embedded binary detection), the 6-pass scanning architecture, how rules are matched against different file types, custom rules support via --custom-rules, rule validation with validate-rules command, and the pipeline analyzer for shell command taint analysis. Reference skill_scanner/core/analyzers/static.py, skill_scanner/core/analyzers/pipeline_analyzer.py, skill_scanner/data/rules/signatures.yaml, skill_scanner/data/yara_rules/, skill_scanner/core/rules/.",
      "parent": "Analyzers"
    },
    {
      "title": "Behavioral Analyzer",
      "purpose": "Deep dive into Python AST dataflow analysis: how the behavioral analyzer works without executing code, components (AST parser, CFG builder, forward dataflow tracker, taint tracker, interprocedural analysis, cross-file analysis, name resolver, type analysis, bash taint tracker), how taint sources and sinks are defined, detection of data exfiltration paths, command injection through eval/exec, and multi-file analysis. Include examples of what it catches that static analysis misses. Reference skill_scanner/core/analyzers/behavioral_analyzer.py, skill_scanner/core/static_analysis/, docs/behavioral-analyzer.md.",
      "parent": "Analyzers"
    },
    {
      "title": "LLM Analyzer",
      "purpose": "Deep dive into LLM-as-a-judge analysis: how SKILL.md content is sent to LLMs for semantic analysis, supported providers (Anthropic Claude, OpenAI GPT, LiteLLM, Google GenAI, AWS Bedrock, GCP Vertex AI, Azure OpenAI), prompt construction and templates in skill_scanner/data/prompts/, structured output parsing, consensus mode with --llm-consensus-runs, threat-to-AITech taxonomy mapping, provider-specific configuration, and cost/latency considerations. Reference skill_scanner/core/analyzers/llm_analyzer.py, skill_scanner/core/analyzers/llm_prompt_builder.py, skill_scanner/data/prompts/, docs/llm-analyzer.md.",
      "parent": "Analyzers"
    },
    {
      "title": "Meta-Analyzer and External Analyzers",
      "purpose": "Combined documentation for the meta-analyzer and external service analyzers: (1) Meta-Analyzer — second-pass LLM analysis for false positive filtering, consolidation of redundant findings, severity normalization, 64% noise reduction, --enable-meta flag, SKILL_SCANNER_META_LLM_* env vars; (2) VirusTotal Analyzer — hash-based malware detection for binary files, VIRUSTOTAL_API_KEY setup, file upload option, --use-virustotal; (3) AI Defense Analyzer — Cisco cloud-based threat detection, AI_DEFENSE_API_KEY/AI_DEFENSE_API_URL setup, default rules, --use-aidefense; (4) Trigger Analyzer — description specificity checking, --use-trigger; (5) Bytecode Analyzer — .pyc integrity checking. Reference skill_scanner/core/analyzers/meta_analyzer.py, skill_scanner/core/analyzers/virustotal_analyzer.py, skill_scanner/core/analyzers/aidefense_analyzer.py, skill_scanner/core/analyzers/trigger_analyzer.py, skill_scanner/core/analyzers/bytecode_analyzer.py, docs/meta-analyzer.md, docs/aidefense-analyzer.md.",
      "parent": "Analyzers"
    },
    {
      "title": "Threat Taxonomy",
      "purpose": "Complete threat taxonomy documentation: all 11 threat categories with severity levels (CRITICAL: Command/Code Injection AITech-9.1.4, Data Exfiltration AITech-8.2/8.2.3, Hardcoded Secrets AITech-8.2; HIGH: Prompt Injection AITech-1.1/1.2, Transitive Trust AITech-1.2, Tool Chaining AITech-8.2.3; MEDIUM: Autonomy Abuse AITech-9.1, Tool/Permission Abuse AITech-12.1; LOW: Social Engineering AITech-2.1, Resource Abuse AITech-13.3.2, Obfuscation), the Cisco Integrated AI Security Framework mapping, MITRE ATLAS alignment, OWASP LLM Top 10 mapping, NIST AI RMF references, ThreatMapping class, and how findings are categorized. Reference skill_scanner/threats/threats.py, skill_scanner/threats/cisco_ai_taxonomy.py, docs/threat-taxonomy.md.",
      "parent": "Architecture"
    },
    {
      "title": "Development",
      "purpose": "Parent page for development documentation. Overview of the development workflow, tooling (uv, pytest, ruff, mypy), project structure, and how to contribute to the project.",
      "parent": "Home"
    },
    {
      "title": "Development Setup and Testing",
      "purpose": "How to set up a development environment and run tests: prerequisites (Python 3.10+, uv), cloning the repo, uv sync for dependency installation, pre-commit install, code quality tools (ruff linting and formatting, mypy type checking). Testing infrastructure: pytest configuration, test markers (slow, integration, requires_llm, e2e), unit test organization in tests/ directory, test fixtures in conftest.py, running specific test suites, coverage reporting with Codecov, and how to write new tests. Reference docs/developing.md, pyproject.toml, .pre-commit-config.yaml, tests/.",
      "parent": "Development"
    },
    {
      "title": "Evaluation Framework",
      "purpose": "The eval framework for measuring detection accuracy: eval skills in evals/skills/ organized by threat category (backdoor/magic-string-trigger, behavioral-analysis/multi-file-exfiltration, command-injection/eval-execution, data-exfiltration/environment-secrets, obfuscation/base64-payload, path-traversal/file-reader, prompt-injection/jailbreak-override, resource-exhaustion/infinite-loop, safe-skills, sql-injection/database-query), _expected.json format for ground truth, benchmark_runner.py for accuracy/precision/recall/F1 metrics, eval_runner.py for per-skill evaluation, policy_benchmark.py for comparing policy presets, update_expected_findings.py for updating baselines, the 10 policy presets in evals/policies/ (baseline, strict, permissive, compliance, CI, internal tooling, etc.), and how to run benchmarks. Reference evals/.",
      "parent": "Development"
    },
    {
      "title": "CI/CD Setup and Integration",
      "purpose": "Complete CI/CD guide: (1) GitHub Actions workflows — python-tests.yml (lint with pre-commit, test across Python 3.10-3.12, benchmark, coverage with Codecov, pip-audit, liccheck), integration-tests.yml (LLM/VT/AI Defense tests with GitHub secrets, triggered by label or manual), release.yml (PyPI publishing with trusted publishing); (2) Integrating skill-scanner into your own CI/CD — using --fail-on-findings for build gates, SARIF output for GitHub Code Scanning integration, JSON output for automation, exit codes; (3) Docker deployment example; (4) Batch scanning in CI with scan-all. Include real workflow YAML examples. Reference .github/workflows/, examples/integration_example.py, docs/api-server.md.",
      "parent": "Development"
    },
    {
      "title": "Examples and How-To Guides",
      "purpose": "Walkthrough of all 8 example scripts in examples/ with code and expected outputs: (1) basic_scan.py — simplest single-skill scan with StaticAnalyzer; (2) programmatic_usage.py — SDK usage with custom config and finding grouping by category/severity; (3) advanced_scanning.py — multi-analyzer setup with severity filtering and JSON/Markdown output; (4) batch_scanning.py — recursive directory scanning with JSON reports and severity breakdown; (5) behavioral_analyzer_example.py — static vs behavioral vs combined comparison; (6) llm_analyzer_example.py — LLM analysis with Claude/GPT, static vs LLM comparison; (7) api_usage.py — REST API usage with httpx (health, scan, upload, batch); (8) integration_example.py — CI/CD patterns with fail-on-critical, fail-on-high, fail-on-findings. Reference examples/.",
      "parent": "Home"
    },
    {
      "title": "Reference",
      "purpose": "Parent page for reference documentation. Index of all reference materials: configuration options, API endpoints, output formats, dependencies, CLI commands, and detection rule formats.",
      "parent": "Home"
    },
    {
      "title": "Configuration Reference",
      "purpose": "Complete reference table of all configuration options: every environment variable with description, default value, and what it enables (SKILL_SCANNER_LLM_API_KEY, SKILL_SCANNER_LLM_MODEL, SKILL_SCANNER_LLM_PROVIDER, SKILL_SCANNER_LLM_BASE_URL, SKILL_SCANNER_LLM_API_VERSION, SKILL_SCANNER_META_LLM_*, VIRUSTOTAL_API_KEY, VIRUSTOTAL_UPLOAD_FILES, AI_DEFENSE_API_KEY, AI_DEFENSE_API_URL, ENABLE_*_ANALYZER, AWS_REGION, AWS_PROFILE); every CLI flag; scan policy YAML schema with all sections; .skill_scannerrc format; and configuration precedence (CLI > env > defaults). Reference skill_scanner/config/config.py, skill_scanner/config/constants.py, skill_scanner/core/scan_policy.py.",
      "parent": "Reference"
    },
    {
      "title": "API Endpoint Reference",
      "purpose": "Detailed API reference for every REST endpoint with request/response JSON schemas: GET / (service info, version, docs links), GET /health (available analyzers), POST /scan (ScanRequest: skill_path, policy, use_behavioral, use_llm, etc.), POST /scan-upload (multipart ZIP upload with query params), POST /scan-batch (BatchScanRequest for background batch), GET /scan-batch/{scan_id} (status: pending/running/completed/failed with results), GET /analyzers (capabilities list). Include curl and httpx examples. Reference skill_scanner/api/router.py, skill_scanner/api/api.py.",
      "parent": "Reference"
    },
    {
      "title": "Output Formats",
      "purpose": "Documentation of all five output reporters with example output for each: (1) Summary (default) — colored console with severity indicators, finding details, remediation advice; (2) JSON — full schema with findings array, compact mode with --compact; (3) Markdown — table-based report with severity, category, description, evidence; (4) Table — ASCII table for terminal display; (5) SARIF 2.1.0 — GitHub Code Scanning compatible, how to upload with github/codeql-action/upload-sarif. Reference skill_scanner/core/reporters/.",
      "parent": "Reference"
    },
    {
      "title": "Dependencies and LLM Providers",
      "purpose": "Complete dependency documentation: core dependencies (FastAPI, Pydantic v2, PyYAML, python-frontmatter, yara-python, httpx, click, rich, textual), dev dependencies (pytest, pytest-asyncio, ruff, mypy). LLM provider setup guide for each of the 7 supported providers: Anthropic Claude (anthropic SDK), OpenAI GPT (openai SDK), LiteLLM (unified 100+ models), Google GenAI (google-generativeai), AWS Bedrock (boto3, [bedrock] extra), GCP Vertex AI (google-cloud-aiplatform, [vertex] extra), Azure OpenAI (azure-identity, [azure] extra). Installation extras syntax and version compatibility. Reference pyproject.toml.",
      "parent": "Reference"
    },
    {
      "title": "CLI Command Reference",
      "purpose": "Quick-reference for all CLI commands and flags in concise table format: skill-scanner scan (all flags with types and defaults), skill-scanner scan-all (--recursive, --check-overlap), skill-scanner list-analyzers, skill-scanner validate-rules (--rules-file), skill-scanner generate-policy (--preset, -o), skill-scanner configure-policy, skill-scanner-api (--host, --port, --reload), and skill-scanner-pre-commit (install, --severity, --skills-path, --all). Reference skill_scanner/cli/cli.py, skill_scanner/api/api_cli.py, skill_scanner/hooks/pre_commit.py.",
      "parent": "Reference"
    }
  ]
}
