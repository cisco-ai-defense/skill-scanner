name: Integration Tests (External APIs)

# This workflow runs tests that require external API keys (LLM, AI Defense, VirusTotal).
# It only runs when:
# 1. Manually triggered via workflow_dispatch
# 2. A PR has the 'run-integration-tests' label (checked on open, push, reopen, or label add)
#
# This prevents resource exhaustion and protects API keys from untrusted PRs.

on:
  workflow_dispatch:
    inputs:
      test_llm:
        description: 'Run LLM analyzer tests'
        required: false
        type: boolean
        default: true
      test_aidefense:
        description: 'Run AI Defense analyzer tests'
        required: false
        type: boolean
        default: true
      test_virustotal:
        description: 'Run VirusTotal analyzer tests'
        required: false
        type: boolean
        default: true
  pull_request:
    types: [labeled, opened, synchronize, reopened]

jobs:
  check-label:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - id: check
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]] || \
             [[ "${{ contains(github.event.pull_request.labels.*.name, 'run-integration-tests') }}" == "true" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "Skipping integration tests â€” 'run-integration-tests' label not present"
          fi

  integration-tests:
    runs-on: ubuntu-latest
    needs: check-label
    if: needs.check-label.outputs.should_run == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python for uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run LLM analyzer tests
        if: github.event_name == 'workflow_dispatch' && inputs.test_llm || github.event_name != 'workflow_dispatch'
        env:
          # Map secrets to SKILL_SCANNER_* environment variables
          # GEMINI_API_KEY is used as fallback if SKILL_SCANNER_LLM_API_KEY is not set
          SKILL_SCANNER_LLM_API_KEY: ${{ secrets.SKILL_SCANNER_LLM_API_KEY || secrets.GEMINI_API_KEY }}
          SKILL_SCANNER_LLM_MODEL: ${{ secrets.SKILL_SCANNER_LLM_MODEL }}
          SKILL_SCANNER_LLM_BASE_URL: ${{ secrets.SKILL_SCANNER_LLM_BASE_URL }}
          SKILL_SCANNER_LLM_API_VERSION: ${{ secrets.SKILL_SCANNER_LLM_API_VERSION }}
        run: |
          if [ -n "$SKILL_SCANNER_LLM_API_KEY" ]; then
            echo "Running LLM analyzer tests..."
            uv run pytest tests/test_llm_analyzer.py -v --tb=short
          else
            echo "Skipping LLM tests - no API keys configured"
          fi

      - name: Run AI Defense analyzer tests
        if: github.event_name == 'workflow_dispatch' && inputs.test_aidefense || github.event_name != 'workflow_dispatch'
        env:
          AI_DEFENSE_API_KEY: ${{ secrets.AI_DEFENSE_API_KEY }}
        run: |
          if [ -n "$AI_DEFENSE_API_KEY" ]; then
            echo "Running AI Defense analyzer tests..."
            uv run pytest tests/test_aidefense_analyzer.py -v --tb=short -k "not mock"
          else
            echo "Skipping AI Defense tests - no API key configured"
          fi

      - name: Run VirusTotal analyzer tests
        if: github.event_name == 'workflow_dispatch' && inputs.test_virustotal || github.event_name != 'workflow_dispatch'
        env:
          VIRUSTOTAL_API_KEY: ${{ secrets.VIRUSTOTAL_API_KEY }}
        run: |
          if [ -n "$VIRUSTOTAL_API_KEY" ]; then
            echo "Running VirusTotal analyzer tests..."
            uv run pytest tests/test_virustotal_analyzer.py tests/test_virustotal_upload.py tests/test_virustotal_benign.py -v --tb=short -k "not mock"
          else
            echo "Skipping VirusTotal tests - no API key configured"
          fi

      - name: Run full evaluation with LLM
        if: github.event_name == 'workflow_dispatch' && inputs.test_llm || github.event_name != 'workflow_dispatch'
        env:
          # Map secrets to SKILL_SCANNER_* environment variables
          # GEMINI_API_KEY is used as fallback if SKILL_SCANNER_LLM_API_KEY is not set
          SKILL_SCANNER_LLM_API_KEY: ${{ secrets.SKILL_SCANNER_LLM_API_KEY || secrets.GEMINI_API_KEY }}
          SKILL_SCANNER_LLM_MODEL: ${{ secrets.SKILL_SCANNER_LLM_MODEL }}
          SKILL_SCANNER_LLM_BASE_URL: ${{ secrets.SKILL_SCANNER_LLM_BASE_URL }}
          SKILL_SCANNER_LLM_API_VERSION: ${{ secrets.SKILL_SCANNER_LLM_API_VERSION }}
        run: |
          if [ -n "$SKILL_SCANNER_LLM_API_KEY" ]; then
            echo "Running evaluation with LLM analyzer..."
            uv run python evals/runners/eval_runner.py --test-skills-dir evals/skills --use-llm
          else
            echo "Skipping LLM evaluation - no API key configured"
          fi

  # This job always runs so it can be used as a required status check.
  # When the label is missing, integration-tests is skipped (which GitHub
  # treats as "not passed" for required checks). This conclusion job
  # reports success unless integration-tests actually failed.
  integration-tests-conclusion:
    if: always()
    needs: [check-label, integration-tests]
    runs-on: ubuntu-latest
    steps:
      - run: |
          if [[ "${{ needs.integration-tests.result }}" == "failure" ]]; then
            echo "Integration tests failed"
            exit 1
          fi
          echo "Integration tests passed or were not required"
